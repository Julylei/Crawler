# sina_image_crawler.py
import requests
import os
import pandas as pd
import time
import urllib3
from bs4 import BeautifulSoup
import re
from playwright.sync_api import sync_playwright
from urllib.parse import urljoin
import hashlib

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


class SinaMilitaryImageCrawler:
    def __init__(self):
        self.images_folder = "test"
        os.makedirs(self.images_folder, exist_ok=True)

        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
            'Referer': 'https://mil.news.sina.com.cn/',
        }

        self.processed_articles = set()
        self.processed_images = set()
        self.image_data = []  # å­˜å‚¨å›¾ç‰‡è·¯å¾„çš„åˆ—è¡¨
        self.image_count = 0
        self.max_images = 100

    def click_load_more_with_playwright(self):
        """ä½¿ç”¨Playwrightè·å–æ–‡ç« """
        print(" ä½¿ç”¨Playwrightè·å–æ–‡ç« ...")

        all_articles = []

        with sync_playwright() as p:
            browser = p.chromium.launch(headless=False)
            context = browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent=self.headers['User-Agent']
            )
            page = context.new_page()

            try:
                page.goto("https://mil.news.sina.com.cn/", timeout=60000)
                time.sleep(5)

                page.wait_for_selector('.ty-cardlist-w', timeout=10000)
                print("   é¡µé¢åŠ è½½å®Œæˆ")

                click_count = 0
                consecutive_failures = 0
                max_consecutive_failures = 5
                max_clicks = 10

                while (click_count < max_clicks and
                       consecutive_failures < max_consecutive_failures and
                       self.image_count < self.max_images):

                    page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    time.sleep(2)

                    current_articles = self.extract_articles_from_playwright_page(page)

                    seen_urls = {article['link'] for article in all_articles}
                    new_articles = [article for article in current_articles if article['link'] not in seen_urls]

                    if new_articles:
                        all_articles.extend(new_articles)
                        print(f"   æ»šåŠ¨è·å–: {len(new_articles)}ç¯‡æ–°æ–‡ç«  (æ€»è®¡: {len(all_articles)}ç¯‡)")
                        consecutive_failures = 0
                    else:
                        consecutive_failures += 1
                        print(f"   æ»šåŠ¨æœªå‘ç°æ–°æ–‡ç« ï¼Œè¿ç»­å¤±è´¥: {consecutive_failures}æ¬¡")

                    if self.image_count >= self.max_images:
                        break

                    clicked = False
                    try:
                        js_script = """
                        (function() {
                            const buttons = [
                                document.querySelector('.cardlist-a__more-c'),
                                document.querySelector('[node-type="cardlist-reload-bottom"]'),
                                document.querySelector('div[data-sudaclick*="feed_refresh"]'),
                                document.querySelector('.load-more'),
                                document.querySelector('.more-btn'),
                                document.querySelector('.ty-card-ft-more')
                            ].filter(btn => btn !== null);

                            for (let btn of buttons) {
                                try {
                                    btn.scrollIntoView({behavior: 'smooth', block: 'center'});
                                    const rect = btn.getBoundingClientRect();
                                    const isVisible = rect.top >= 0 && rect.left >= 0 && 
                                                    rect.bottom <= (window.innerHeight || document.documentElement.clientHeight) && 
                                                    rect.right <= (window.innerWidth || document.documentElement.clientWidth);

                                    if (isVisible) {
                                        btn.click();
                                        return true;
                                    } else {
                                        const clickEvent = new MouseEvent('click', {
                                            bubbles: true,
                                            cancelable: true,
                                            view: window,
                                            buttons: 1
                                        });
                                        btn.dispatchEvent(clickEvent);
                                        return true;
                                    }
                                } catch (e) {
                                    continue;
                                }
                            }
                            return false;
                        })();
                        """
                        result = page.evaluate(js_script)
                        if result:
                            clicked = True
                            click_count += 1
                            print(f"   ç‚¹å‡»æˆåŠŸ (ç¬¬{click_count}æ¬¡)")

                            print(f"   ç­‰å¾…æ–°å†…å®¹åŠ è½½...")
                            time.sleep(5)

                            for i in range(3):
                                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                                time.sleep(2)
                                print(f"   æ»šåŠ¨åŠ è½½ç¬¬{i + 1}æ¬¡")

                            time.sleep(3)

                            post_click_articles = self.extract_articles_from_playwright_page(page)
                            post_seen_urls = {article['link'] for article in all_articles}
                            post_new_articles = [article for article in post_click_articles if
                                                 article['link'] not in post_seen_urls]

                            if post_new_articles:
                                all_articles.extend(post_new_articles)
                                print(f"   ç‚¹å‡»åè·å–: {len(post_new_articles)}ç¯‡æ–°æ–‡ç«  (æ€»è®¡: {len(all_articles)}ç¯‡)")

                            consecutive_failures = 0

                    except Exception as e:
                        print(f"   ç‚¹å‡»å‡ºé”™: {e}")

                    if not clicked:
                        consecutive_failures += 1
                        print(f"   æœªæ‰¾åˆ°å¯ç‚¹å‡»æŒ‰é’®ï¼Œè¿ç»­å¤±è´¥: {consecutive_failures}æ¬¡")

                    time.sleep(2)
                    print(
                        f"   å½“å‰è¿›åº¦: æ–‡ç« {len(all_articles)}ç¯‡, å›¾ç‰‡{self.image_count}/{self.max_images}å¼ , ç‚¹å‡»æ¬¡æ•°: {click_count}/{max_clicks}")

                print(f"   æœ€ç»ˆè·å–: {len(all_articles)}ç¯‡æ–‡ç« ")
                print(f"   æ€»ç‚¹å‡»æ¬¡æ•°: {click_count}æ¬¡")

            except Exception as e:
                print(f"   Playwrightæ‰§è¡Œå‡ºé”™: {e}")
            finally:
                browser.close()

        return all_articles

    def extract_articles_from_playwright_page(self, page):
        """ä»Playwrighté¡µé¢æå–æ–‡ç« ä¿¡æ¯"""
        try:
            html_content = page.content()
            soup = BeautifulSoup(html_content, 'html.parser')

            articles = []

            selectors = [
                '.ty-cardlist-w .ty-card',
                '.ty-card',
                '.news-item',
                '.news-list li',
                '.feed-card-item',
                '[data-sudaclick*="news"]'
            ]

            news_items = []
            for selector in selectors:
                items = soup.select(selector)
                if items:
                    news_items.extend(items)
                    break

            if not news_items:
                news_items = soup.find_all('div', class_=re.compile(r'card|news|item'))

            for item in news_items:
                try:
                    link = item.find('a', href=True)
                    if not link:
                        continue

                    url = link.get('href')
                    if not url:
                        continue

                    if not url.startswith('http'):
                        url = urljoin('https://mil.news.sina.com.cn/', url)

                    if not any(domain in url for domain in ['.sina.com.cn', '.sina.cn']):
                        continue

                    title = ""
                    for tag in ['h1', 'h2', 'h3', 'h4']:
                        title_elem = item.find(tag)
                        if title_elem:
                            title_text = title_elem.get_text().strip()
                            if title_text and len(title_text) > 5:
                                title = title_text
                                break

                    if not title:
                        title_text = link.get_text().strip()
                        if title_text and len(title_text) > 5:
                            title = title_text

                    if not title or len(title) < 5:
                        continue

                    articles.append({
                        'title': title,
                        'link': url,
                    })

                except Exception:
                    continue

            return articles

        except Exception:
            return []

    def get_article_content(self, article_url):
        """è·å–æ–‡ç« è¯¦æƒ…é¡µå†…å®¹"""
        try:
            response = requests.get(article_url, headers=self.headers, timeout=10, verify=False)
            response.encoding = 'utf-8'
            return response.text if response.status_code == 200 else None
        except Exception:
            return None

    def download_image(self, img_url):
        """ä¸‹è½½å›¾ç‰‡"""
        if self.image_count >= self.max_images:
            return None

        try:
            img_hash = hashlib.md5(img_url.encode()).hexdigest()
            if img_hash in self.processed_images:
                return None

            img_headers = self.headers.copy()
            img_headers['Accept'] = 'image/webp,image/apng,image/*,*/*;q=0.8'

            response = requests.get(img_url, headers=img_headers, timeout=10, verify=False)
            if response.status_code == 200:
                if len(response.content) < 5000:
                    return None

                file_extension = os.path.splitext(img_url.split('?')[0])[1]
                if not file_extension or len(file_extension) > 5:
                    file_extension = '.jpg'

                filename = f"{self.image_count + 1:03d}{file_extension}"
                filepath = os.path.join(self.images_folder, filename)

                with open(filepath, 'wb') as f:
                    f.write(response.content)

                self.processed_images.add(img_hash)
                self.image_count += 1

                # ç›´æ¥å­˜å‚¨ç»å¯¹è·¯å¾„å­—ç¬¦ä¸²
                absolute_path = os.path.abspath(filepath)
                return absolute_path

        except Exception as e:
            print(f"    å›¾ç‰‡ä¸‹è½½å¤±è´¥: {e}")

        return None

    def extract_images_from_article(self, html_content, article_title):
        """ä»æ–‡ç« é¡µé¢æå–å›¾ç‰‡"""
        if self.image_count >= self.max_images:
            return []

        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            image_paths = []

            img_selectors = [
                'div.article-content img',
                'div.article-body img',
                'div#artibody img',
                'div.content img',
                'div.main-content img',
                'div.article img'
            ]

            for selector in img_selectors:
                img_tags = soup.select(selector)
                for img_tag in img_tags:
                    if self.image_count >= self.max_images:
                        break

                    img_src = img_tag.get('src') or img_tag.get('data-src') or img_tag.get('data-original')
                    if not img_src:
                        continue

                    if not img_src.startswith('http'):
                        img_src = urljoin('https://mil.news.sina.com.cn/', img_src)

                    if any(keyword in img_src.lower() for keyword in ['icon', 'logo', 'spacer', 'ad', 'gif']):
                        continue

                    if 'sina.com.cn/images' in img_src:
                        continue

                    image_path = self.download_image(img_src)
                    if image_path:
                        image_paths.append(image_path)
                        print(f"    ğŸ“· ä¸‹è½½å›¾ç‰‡: {self.image_count:03d}.jpg")

            return image_paths

        except Exception as e:
            print(f"    æå–å›¾ç‰‡å¤±è´¥: {e}")
            return []

    def crawl_images(self):
        """çˆ¬å–å›¾ç‰‡"""
        print(f"ç›®æ ‡: {self.max_images}å¼ å›¾ç‰‡")
        print("è·å–æ–‡ç« ä¸­...")

        articles = self.click_load_more_with_playwright()

        if not articles:
            print("æŠ±æ­‰ï¼Œæ²¡æœ‰è·å–åˆ°æ–‡ç« ")
            return 0

        print(f"å¼€å§‹å¤„ç† {len(articles)} ç¯‡æ–‡ç« ä¸­çš„å›¾ç‰‡...")

        processed_urls = set()
        processed_titles = set()

        for i, article in enumerate(articles):
            if self.image_count >= self.max_images:
                break

            title = article.get('title', '').strip()
            article_url = article.get('link', '')

            if not title or not article_url:
                continue

            if article_url.startswith('//'):
                article_url = 'https:' + article_url

            if article_url in processed_urls:
                continue

            title_key = re.sub(r'[^\w\u4e00-\u9fa5]', '', title.lower())
            if title_key in processed_titles:
                continue

            processed_urls.add(article_url)
            processed_titles.add(title_key)

            print(f"  [{i + 1:2d}/{len(articles)}] å¤„ç†æ–‡ç« : {title[:30]}...")

            html_content = self.get_article_content(article_url)
            if not html_content:
                print(f"    è·å–å†…å®¹å¤±è´¥")
                continue

            image_paths = self.extract_images_from_article(html_content, title)

            if image_paths:
                # ç›´æ¥æ‰©å±•è·¯å¾„åˆ—è¡¨
                self.image_data.extend(image_paths)
                print(f"    ä»æ­¤æ–‡ç« è·å– {len(image_paths)} å¼ å›¾ç‰‡")

            print(f"    è¿›åº¦: {self.image_count}/{self.max_images} å¼ å›¾ç‰‡")
            time.sleep(0.5)

        return self.image_count

    def save_to_excel(self):
        """å°†å›¾ç‰‡ä¿¡æ¯ä¿å­˜åˆ°Excel"""
        if not self.image_data:
            print("   è­¦å‘Š: æ²¡æœ‰å›¾ç‰‡æ•°æ®å¯ä¿å­˜åˆ°Excel")
            return None

        try:
            # åˆ›å»ºDataFrameï¼ŒåªåŒ…å«ç»å¯¹è·¯å¾„
            df = pd.DataFrame({
                'absolute_path': self.image_data
            })

            excel_filename = "test.xlsx"
            excel_path = os.path.join(os.getcwd(), excel_filename)

            # ä¿å­˜Excelæ–‡ä»¶
            df.to_excel(excel_path, index=False)
            print(f"   Excelæ–‡ä»¶å·²ä¿å­˜: {excel_path}")
            return os.path.abspath(excel_path)

        except Exception as e:
            print(f"   ä¿å­˜Excelå¤±è´¥: {e}")
            return None

    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        print("=== æ–°æµªå†›äº‹å›¾ç‰‡çˆ¬è™« ===")
        start_time = time.time()

        image_count = self.crawl_images()
        print(f"çˆ¬å–å®Œæˆï¼Œå¼€å§‹ä¿å­˜Excel...")
        excel_path = self.save_to_excel()

        end_time = time.time()
        elapsed_time = end_time - start_time

        print(f"\nçˆ¬å–å®Œæˆ!")
        print(f"   è€—æ—¶: {elapsed_time:.1f}ç§’")
        print(f"   æˆåŠŸä¸‹è½½: {image_count}å¼ å›¾ç‰‡")
        print(f"   ä¿å­˜è·¯å¾„: {os.path.abspath(self.images_folder)}")

        if excel_path:
            print(f"   Excelè®°å½•: {excel_path}")
        else:
            print(f"   Excelæ–‡ä»¶ç”Ÿæˆå¤±è´¥")

        return {
            'success': image_count > 0,
            'image_count': image_count,
            'excel_path': excel_path,
            'image_folder': os.path.abspath(self.images_folder)
        }


def main():
    """ä¸»å‡½æ•°"""
    crawler = SinaMilitaryImageCrawler()
    result = crawler.run()

    if result['success']:
        print(f"\næˆåŠŸä¸‹è½½ {result['image_count']} å¼ å›¾ç‰‡")
        print(f"å›¾ç‰‡ä¿å­˜åœ¨: {result['image_folder']}")
        if result['excel_path']:
            print(f"Excelè®°å½•: {result['excel_path']}")
    else:
        print(f"\nçˆ¬å–å¤±è´¥")


if __name__ == "__main__":
    main()